# ì‹œì¥ ì´ìŠˆ ìˆ˜ì§‘Â·ìš”ì•½ 
import logging
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from src.agents.finnhub_client import FinnhubClient
from src.config import settings
import datetime
import pytz
import time  # time ëª¨ë“ˆì„ ì „ì—­ì—ì„œ í•œ ë²ˆë§Œ ì„í¬íŠ¸

logger = logging.getLogger(__name__)


logger.info("InfoCrawler ì´ˆê¸°í™” ì™„ë£Œ: Google CSE + Finnhub ê¸°ë°˜ ì •ë³´ ìˆ˜ì§‘ ë° Azure OpenAI ìš”ì•½ ì‚¬ìš©")

class InfoCrawler:
    """
    ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§, ë‰´ìŠ¤ ê²€ìƒ‰ ë“±ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """

    def __init__(self, status_notifier=None):
        # Finnhub + Google Custom Search ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬ì„±
        self.finnhub = FinnhubClient(settings.FINNHUB_API_KEY)
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì½œë°± ("in_progress"/"completed"/"error")
        self.status_notifier = status_notifier
        logger.debug(f"[InfoCrawler.__init__] status_notifier set: {bool(status_notifier)}")

        logger.info("InfoCrawler initialized (Google CSE + Finnhub).")

    def fetch_article_text(self, url: str) -> str:
        """
        ì£¼ì–´ì§„ URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (BeautifulSoup ê¸°ë°˜)
        """
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
            resp = requests.get(url, headers=headers, timeout=12)
            if resp.status_code != 200:
                logger.warning(f"[fetch_article_text] HTTP {resp.status_code} for {url}")
                return ""
            soup = BeautifulSoup(resp.text, "html.parser")
            # ëŒ€í‘œì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            article = soup.find('article')
            if article and article.get_text(strip=True):
                return article.get_text(separator='\n', strip=True)
            main = soup.find('div', id='main')
            if main and main.get_text(strip=True):
                return main.get_text(separator='\n', strip=True)
            # ì—¬ëŸ¬ <p> íƒœê·¸ë¥¼ í•©ì³ì„œ ë³¸ë¬¸ ìƒì„±
            ps = soup.find_all('p')
            text = '\n'.join([p.get_text(strip=True) for p in ps if len(p.get_text(strip=True)) > 30])
            if len(text) > 100:
                return text
            # fallback: ì „ì²´ í…ìŠ¤íŠ¸ ì¤‘ ê¸¸ì´ê°€ ê¸´ ë¶€ë¶„
            body = soup.find('body')
            if body:
                all_text = body.get_text(separator='\n', strip=True)
                if len(all_text) > 100:
                    return all_text
            return ""
        except Exception as e:
            logger.error(f"[fetch_article_text] Error fetching article from {url}: {e}", exc_info=True)
            return ""


        # (ë¶ˆí•„ìš”í•œ import time êµ¬ë¬¸ ì‚­ì œë¨)

    def get_market_summary(self, user_query: str, max_articles: int = 10) -> str:
        last_status_time = 0
        def throttled_notify(msg):
            nonlocal last_status_time
            now = time.time()
            elapsed = now - last_status_time
            logger.debug(f"[throttled_notify] elapsed={elapsed:.3f}s for '{msg}'")
            if elapsed > 1:
                if self.status_notifier:
                    try:
                        logger.debug(f"[throttled_notify] sending '{msg}'")
                        self.status_notifier(msg)
                    except Exception as e:
                        logger.error(f"[throttled_notify] notifier error for '{msg}': {e}", exc_info=True)
                else:
                    logger.debug(f"[throttled_notify] no notifier registered for '{msg}'")
                last_status_time = now
            else:
                logger.debug(f"[throttled_notify] skipping '{msg}' due to throttling")
        # ë‹¨ê³„ ì‹œì‘
        logger.debug("[get_market_summary] ì‹œì‘: market summary for query='%s'", user_query)
        throttled_notify("ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘")
        logger.info(f"[get_market_summary] called with user_query='{user_query}' max_articles={max_articles}")
        logger.info(f"Getting market summary for query: '{user_query}'")
        
        # Fetch web results for the user query (web search only)
        if user_query:
            logger.info(f"Searching web results for query: {user_query}")
        else:
            logger.info("Searching general web results as no specific query provided.")
        # Google ê²€ìƒ‰ API ì§ì ‘ í˜¸ì¶œë¡œ ëŒ€ì²´
        logger.debug(f"[get_market_summary] Starting Google search for query: {user_query}")
        google_results = []
        try:
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                "key": settings.GOOGLE_API_KEY,
                "cx": settings.GOOGLE_CX,
                "q": user_query,
                "num": max_articles,     # max_articles ê°œë§Œ ìš”ì²­
                "dateRestrict": "d7"      # ìµœê·¼ 7ì¼ ì´ë‚´ ê²°ê³¼
            }
            logger.debug(f"[get_market_summary] Google API request params: {params}")
            resp = requests.get(url, params=params, timeout=12)
            logger.debug(f"[get_market_summary] Google API response status: {resp.status_code}")
            if resp.status_code == 200:
                data = resp.json()
                google_results = data.get("items", [])
                logger.info(f"[get_market_summary] Google search returned {len(google_results)} items.")
                if google_results:
                    logger.debug(f"[get_market_summary] First Google result: {google_results[0]}")
            else:
                logger.warning(f"[get_market_summary] Google search error: HTTP {resp.status_code}, content: {resp.text}")
        except Exception as e:
            logger.error(f"[get_market_summary] Google search exception: {e}", exc_info=True)
        # Google ë‰´ìŠ¤ API ê²°ê³¼ (ìµœëŒ€ max_articlesê°œ)
        google_news_list = google_results[:max_articles] if isinstance(google_results, list) else []
        logger.info(f"[get_market_summary] Collected {len(google_news_list)} Google web results.")
        logger.debug(f"[get_market_summary] merged_news size before crawling: {len(google_news_list)}")

        # Finnhub ì¼ë°˜ ë‰´ìŠ¤ ê²°ê³¼: max_articlesê°œë§Œ ì‚¬ìš©
        try:
            finnhub_news_list = self.finnhub.get_general_news(category='general')[:max_articles]
            logger.info(f"[get_market_summary] Collected {len(finnhub_news_list)} Finnhub news results.")
        except Exception as e:
            logger.error(f"[get_market_summary] Finnhub news fetch error: {e}", exc_info=True)
            finnhub_news_list = []

        # ê¸°ì‚¬ ë³‘í•© (ì¤‘ë³µ URL ì œê±°)
        url_set = set()
        merged_news = []
        # Google ë‰´ìŠ¤: url, title/headline, snippet/summary
        for item in google_news_list:
            headline = item.get("title") or item.get("headline") or ""
            summary = item.get("snippet") or item.get("summary") or ""
            url = item.get("link") or item.get("url") or ""
            publisher = item.get("displayLink") or item.get("publisher") or ""
            date = item.get("datePublished", "") or item.get("pubDate", "")
            # Google CSE ê²°ê³¼ëŠ” ë¬¸ìì—´ ë‚ ì§œ, ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if date and isinstance(date, int):
                # í˜¹ì‹œë¼ë„ intë¡œ ë“¤ì–´ì˜¤ë©´ ISO í¬ë§·ìœ¼ë¡œ ë³€í™˜
                date = datetime.datetime.fromtimestamp(date, pytz.timezone('Asia/Seoul')).strftime("%Y-%m-%d %H:%M:%S")
            source = "GoogleCSE"
            news = {
                "headline": headline,
                "summary": summary,
                "url": url,
                "publisher": publisher,
                "date": date,
                "source": source
            }
            if url and url not in url_set:
                url_set.add(url)
                merged_news.append(news)
        # Finnhub ë‰´ìŠ¤: url, headline, summary (ë³¸ë¬¸ì€ fetch_article_textë¡œ í¬ë¡¤ë§)
        for item in finnhub_news_list:
            headline = item.get("headline") or ""
            summary = item.get("summary") or ""
            url = item.get("url") or ""
            publisher = item.get("source") or ""
            date = item.get("datetime", "")
            source = "Finnhub"
            # finnhubì˜ datetimeì€ int(timestamp)ì¼ ìˆ˜ ìˆìŒ
            if isinstance(date, int):
                date = datetime.datetime.fromtimestamp(date, pytz.timezone('Asia/Seoul')).strftime("%Y-%m-%d %H:%M:%S")
            elif date and isinstance(date, str) and date.isdigit():
                # í˜¹ì‹œ ë¬¸ìì—´ ìˆ«ìì¸ ê²½ìš°ë„ ë³€í™˜
                date = datetime.datetime.fromtimestamp(int(date), pytz.timezone('Asia/Seoul')).strftime("%Y-%m-%d %H:%M:%S")
            news = {
                "headline": headline,
                "summary": summary,
                "url": url,
                "publisher": publisher,
                "date": date,
                "source": source
            }
            if url and url not in url_set:
                url_set.add(url)
                merged_news.append(news)

        logger.info(f"[get_market_summary] Total merged news count: {len(merged_news)}")
        logger.debug(f"[get_market_summary] merged_news size after merge: {len(merged_news)}")
        # ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ì•Œë¦¼
        throttled_notify("ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

        # â–¶ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘ ì•Œë¦¼
        throttled_notify("ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘")
        if not merged_news:
            logger.warning("No news collected from Google or Finnhub.")
            return "(ê´€ë ¨ ì›¹ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"

        # ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ìš”ì•½ ì¤€ë¹„
        logger.debug(f"[get_market_summary] fetching article texts for {len(urls)} URLs")
        articles_for_prompt = []
        urls = [item.get("url") for item in merged_news]
        with ThreadPoolExecutor(max_workers=10) as pool:
            future_to_url = {pool.submit(self.fetch_article_text, url): url for url in urls if url}
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    article_text = future.result()
                    item = next((i for i in merged_news if i.get("url") == url), {})
                    headline = item.get("headline", "")
                    summary = item.get("summary", "")
                    publisher = item.get("publisher", "")
                    date = item.get("date", "")
                    source = item.get("source", "")
                    # ê¸°ì‚¬ ë³¸ë¬¸ì´ ì¶©ë¶„íˆ ê¸¸ë©´ ë³¸ë¬¸, ì•„ë‹ˆë©´ summary/headline ì‚¬ìš©
                    if article_text and len(article_text) > 200:
                        content = article_text
                    elif summary:
                        content = summary
                    else:
                        content = headline
                    if headline or content:
                        articles_for_prompt.append(
                            f"ì œëª©: {headline.strip()}\në‚´ìš©: {content.strip()}\nì¶œì²˜: {publisher} ({source})\nURL: {url if url else ''}\në‚ ì§œ: {date}\n---"
                        )
                except Exception as exc:
                    logger.error(f"Subquery '{url}' generated an exception: {exc}", exc_info=True)
        logger.info(f"[get_market_summary] articles_for_prompt length: {len(articles_for_prompt)}; first item: {articles_for_prompt[0] if articles_for_prompt else None}")
        # â–¶ ë³¸ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼
        throttled_notify("ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ")
        if not articles_for_prompt:
            logger.warning("Could not extract usable news article contents.")
            return "(ë‰´ìŠ¤ ë‚´ìš©ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"

        # 1ì°¨ ìš”ì•½: ê¸°ì‚¬ ì „ì²´ë¥¼ í•œ ë²ˆì— LLMì— ë³´ë‚´ ì¤‘ë³µ ì—†ì´ í•µì‹¬ë§Œ ìš”ì•½
        from src.utils.azure_openai import azure_chat_completion
        # 1ì°¨ ìš”ì•½ ì‹œì‘ ì•Œë¦¼
        throttled_notify("ìš”ì•½ ì¤‘")
        # ì‹¤ì‹œê°„ KST ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
        now_kst = datetime.datetime.now(pytz.timezone('Asia/Seoul')).strftime("%Y-%m-%d %H:%M:%S")
        
        system_prompt_1 = (
            f"ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½ AIì…ë‹ˆë‹¤. í˜„ì¬ ì‹œê°ì€ {now_kst} (KST)ì…ë‹ˆë‹¤. "
            f"ì•„ë˜ì— ì—¬ëŸ¬ ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ êµ¬ë¶„ë˜ì–´ ì œê³µë©ë‹ˆë‹¤. ê° ê¸°ì‚¬ë³„ë¡œ ì œëª©, ë‚´ìš©, ì¶œì²˜(ì–¸ë¡ ì‚¬/í”Œë«í¼/URL ë“±), ë‚ ì§œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì„¸ìš”. "
            f"ê° ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ í•œê¸€ë¡œ ì¢…í•©ì ì´ê³  ìì„¸í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”. "
            f"ê¸°ì‚¬ë“¤ì€ ì‹ ë¢°ë„ê°€ ë†’ì€ ìˆœì„œ(ì˜ˆ: ê³µì‹ ê¸ˆìœµ í”Œë«í¼ ê¸°ì‚¬ ìš°ì„ ) ë° ìµœì‹ ìˆœ(ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ë¶€í„°)ìœ¼ë¡œ ì •ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
            f"íŠ¹íˆ ì¤‘ìš”í•œ ê¸°ì‚¬ì™€ ì‹œì¥ì— ì˜í–¥ì´ í° ì´ìŠˆëŠ” ë¨¼ì € ê°•ì¡°í•´ ì£¼ì„¸ìš”. "
            f"ê° ê¸°ì‚¬ë³„ë¡œ ì¶œì²˜(ì–¸ë¡ ì‚¬, í”Œë«í¼ ë˜ëŠ” URL ë“±), ë‚ ì§œë¥¼ ë°˜ë“œì‹œ ëª…í™•íˆ í‘œê¸°í•´ ì£¼ì„¸ìš”. "
            f"ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ í†µí•©í•˜ë˜, ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ëˆ„ë½í•˜ì§€ ë§ˆì„¸ìš”. "
            f"ë‰´ìŠ¤ ì›ë¬¸ì„ ì½ì§€ ì•Šì€ ì‚¬ëŒë„ ì „ì²´ ì‹œì¥ ìƒí™©ì„ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡°ì ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. "
        )
        user_content_1 = '\n'.join(articles_for_prompt)
        messages_1 = [
            {"role": "system", "content": system_prompt_1},
            {"role": "user", "content": user_content_1}
        ]
        resp_1 = azure_chat_completion(settings.AZURE_OPENAI_DEPLOYMENT_GPT4_1_NANO, messages=messages_1, max_tokens=8000, temperature=0.3)
        first_summary = resp_1["choices"][0]["message"]["content"].strip()
        logger.debug(f"[get_market_summary] 1ì°¨ ìš”ì•½ ì™„ë£Œ: articles={len(articles_for_prompt)}, length={len(first_summary)}")
        # â–¶ ìš”ì•½ ì™„ë£Œ ì•Œë¦¼
        throttled_notify("ìš”ì•½ ì™„ë£Œ")
        return first_summary


 # Example Usage
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    if not settings.FINNHUB_API_KEY or not settings.AZURE_OPENAI_API_KEY:
        print("\nWarning: FINNHUB_API_KEY or AZURE_OPENAI_API_KEY not set. Summarization might fail.")

    # (ì‹¤ì œ ì‚¬ìš© ì‹œ status_notifier ì½œë°±ì„ ì •ì˜í•´ì„œ ë„˜ê²¨ì£¼ì„¸ìš”)
    import asyncio
    async def main():
        sent_msg = await some_channel.send("ğŸŸ¡ ì‹œì‘í•©ë‹ˆë‹¤...")
        loop = asyncio.get_running_loop()
        def notifier(key: str):
            mapping = {
                "ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘":      "ğŸŸ¡ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...",
                "ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ":    "âœ… ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!",
                "ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘":    "ğŸŸ¡ ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘...",
                "ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ":  "âœ… ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ!",
                "ìš”ì•½ ì¤‘":          "ğŸŸ¡ ìš”ì•½ ì¤‘...",
                "ìš”ì•½ ì™„ë£Œ":        "âœ… ìš”ì•½ ì™„ë£Œ!"
            }
            if content := mapping.get(key):
                asyncio.run_coroutine_threadsafe(sent_msg.edit(content=content), loop)

        crawler = InfoCrawler(status_notifier=notifier)
        test_query = "ìµœê·¼ ì‹œì¥ ë™í–¥ì€ ì–´ë–¤ê°€ìš”?"
        summary = await loop.run_in_executor(None, crawler.get_market_summary, test_query)
        await sent_msg.edit(content=summary)

    asyncio.run(main())